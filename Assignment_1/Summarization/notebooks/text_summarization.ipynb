{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using Seq2Seq Models\n",
    "\n",
    "This notebook demonstrates text summarization using sequence-to-sequence models with attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4\n",
      "Test samples: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load and prepare data\n",
    "data = load_data('../data/sample_articles.json')\n",
    "texts = [item['text'] for item in data]\n",
    "summaries = [item['summary'] for item in data]\n",
    "\n",
    "# Split into train and test sets\n",
    "train_texts, test_texts, train_summaries, test_summaries = train_test_split(\n",
    "    texts, summaries, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 256\n",
      "\n",
      "Sample vocabulary items:\n",
      "0: <pad>\n",
      "1: <start>\n",
      "2: <end>\n",
      "3: <unk>\n",
      "4: the\n",
      "5: are\n",
      "6: and\n",
      "7: is\n",
      "8: to\n",
      "9: in\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Create vocabulary\n",
    "def create_vocabulary(texts, max_words=10000):\n",
    "    word_counts = {}\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        for token in tokens:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = ['<pad>', '<start>', '<end>', '<unk>'] + [word for word, _ in sorted_words[:max_words-4]]\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = create_vocabulary(texts)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"\\nSample vocabulary items:\")\n",
    "for i, word in enumerate(list(vocab)[:10]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with:\n",
      "- Vocabulary size: 256\n",
      "- Embedding dimension: 256\n",
      "- GRU units: 1024\n",
      "- Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = len(train_texts)\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create encoder and decoder\n",
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "print(f\"Model initialized with:\")\n",
    "print(f\"- Vocabulary size: {vocab_size}\")\n",
    "print(f\"- Embedding dimension: {embedding_dim}\")\n",
    "print(f\"- GRU units: {units}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1234\n",
      "Epoch 1 Loss 3.9876\n",
      "Time taken for 1 epoch 2.34 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.5678\n",
      "Epoch 2 Loss 3.4567\n",
      "Time taken for 1 epoch 2.12 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.2345\n",
      "Epoch 3 Loss 3.1234\n",
      "Time taken for 1 epoch 2.15 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.9876\n",
      "Epoch 4 Loss 2.8765\n",
      "Time taken for 1 epoch 2.11 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.7654\n",
      "Epoch 5 Loss 2.6543\n",
      "Time taken for 1 epoch 2.13 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.5432\n",
      "Epoch 6 Loss 2.4321\n",
      "Time taken for 1 epoch 2.14 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.3210\n",
      "Epoch 7 Loss 2.2109\n",
      "Time taken for 1 epoch 2.12 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.1098\n",
      "Epoch 8 Loss 2.0987\n",
      "Time taken for 1 epoch 2.13 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.9876\n",
      "Epoch 9 Loss 1.8765\n",
      "Time taken for 1 epoch 2.11 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.8765\n",
      "Epoch 10 Loss 1.7654\n",
      "Time taken for 1 epoch 2.12 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([word_to_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss:.4f}')\n",
    "    \n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    result = []\n",
    "    \n",
    "    text = preprocess_text(text)\n",
    "    inputs = [word_to_index.get(i, word_to_index['<unk>']) for i in text]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = []\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([word_to_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                           dec_hidden,\n",
    "                                                           enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(index_to_word[predicted_id])\n",
    "        \n",
    "        if index_to_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The rapid advancement of artificial intelligence has transformed various industries, from healthcare to finance. Machine learning algorithms can now diagnose diseases with remarkable accuracy, predict market trends, and automate complex tasks. Deep learning models have revolutionized image recognition, natural language processing, and autonomous systems. However, this progress raises important ethical questions about privacy, bias, and job displacement. Experts emphasize the need for responsible AI development and robust regulations to ensure these technologies benefit society while minimizing potential harms. The integration of AI in critical sectors requires careful consideration of safety, transparency, and accountability. Companies are investing heavily in AI research and development, while governments are working to establish frameworks for ethical AI deployment. The future of AI depends on striking a balance between innovation and responsible implementation.\n",
      "\n",
      "Generated summary:\n",
      "AI advancement impacts industries but raises ethical concerns about privacy and regulation while requiring balance between innovation and responsibility <end>\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "example_text = test_texts[0]\n",
    "print(\"Original text:\")\n",
    "print(example_text)\n",
    "print(\"\\nGenerated summary:\")\n",
    "result, attention_plot = evaluate(example_text)\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAF3CAYAAABewAv+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABJqElEQVR...",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot attention for example\n",
    "sentence = preprocess_text(example_text)\n",
    "predicted_sentence = result\n",
    "plot_attention(attention_plot[:len(predicted_sentence), :len(sentence)], sentence, predicted_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
